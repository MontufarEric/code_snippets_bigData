{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to SparkContext, for SparkSQL you need a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "# Also all the functions (select, where, groupby) needs to be imported\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into dataframe\n",
    "ratings_df = spark.read.csv(\"/home/fieldengineer/Documents/courses/architect_big_data_solutions_with_spark-master/Datasets/movielens/ratings.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames Operations\n",
    "\n",
    "In this part you will learn how to programmatically use the SQL capabilities of DataFrame. For the full list of documentation: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|movieId|rating|\n",
      "+-------+------+\n",
      "|     31|   2.5|\n",
      "|   1029|   3.0|\n",
      "|   1061|   3.0|\n",
      "|   1129|   2.0|\n",
      "|   1172|   4.0|\n",
      "|   1263|   2.0|\n",
      "|   1287|   2.0|\n",
      "|   1293|   2.0|\n",
      "|   1339|   3.5|\n",
      "|   1343|   2.0|\n",
      "|   1371|   2.5|\n",
      "|   1405|   1.0|\n",
      "|   1953|   4.0|\n",
      "|   2105|   4.0|\n",
      "|   2150|   3.0|\n",
      "|   2193|   2.0|\n",
      "|   2294|   2.0|\n",
      "|   2455|   2.5|\n",
      "|   2968|   1.0|\n",
      "|   3671|   3.0|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can use the select method to grab specific columns\n",
    "ratings_df.select(['movieId','rating']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see how ratings are in string\n",
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType \n",
    "ratings_df = ratings_df.withColumn('rating', ratings_df['rating'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can change the Data type of any column by casting them to your desired data type\n",
    "# First you have to import that data type from pyspark.sql.types\n",
    "from pyspark.sql.types import IntegerType\n",
    "# Then you can use withColumn() to apply / cast each row of the column (Notice how the square bracket annotation is used)\n",
    "ratings_df = ratings_df.withColumn(\"rating\", ratings_df['rating'].cast(IntegerType()))\n",
    "# take a look at the schema now\n",
    "ratings_df.select(['movieId','rating']).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use the filter() here to filter on a condition (just like we did with RDD!)\n",
    "# For example we can check if there are any missing ratings \n",
    "ratings_df.filter(ratings_df.rating.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar to filter you can also use where (from SQL syntax)\n",
    "ratings_df.where(ratings_df.rating.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "The GROUP BY statement is used with **aggregate functions (COUNT, MAX, MIN, SUM, AVG)** to group the result-set by one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|movieId|        avg_rating|reviews|\n",
      "+-------+------------------+-------+\n",
      "|   2294| 3.150943396226415|     53|\n",
      "|    296| 4.157407407407407|    324|\n",
      "|   3210|               3.5|     52|\n",
      "|  88140|3.4545454545454546|     22|\n",
      "| 115713| 3.769230769230769|     26|\n",
      "|  27317|3.6666666666666665|      6|\n",
      "|   1090|3.8088235294117645|     68|\n",
      "|   2088|              2.52|     25|\n",
      "|   2136| 2.611111111111111|     18|\n",
      "|   6240|               3.0|      2|\n",
      "|   3959|             3.375|     16|\n",
      "|   3606| 4.111111111111111|      9|\n",
      "|   6731|3.1666666666666665|      6|\n",
      "|  62912|2.6666666666666665|      3|\n",
      "|  89864|3.5384615384615383|     13|\n",
      "| 106022|               4.0|      1|\n",
      "|  48738| 3.933333333333333|     15|\n",
      "|   2069|3.7777777777777777|      9|\n",
      "|   3414|               4.0|      4|\n",
      "|   2162|2.5555555555555554|      9|\n",
      "+-------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For instance, we can group by the movieId over rating and aggregate over the average value and total reviews (very easily)\n",
    "ratings_df.groupBy('movieId').agg(avg('rating').alias('avg_rating'), count('rating').alias('reviews')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|movieId|        avg_rating|reviews|\n",
      "+-------+------------------+-------+\n",
      "|    318| 4.405144694533762|    311|\n",
      "|    858|             4.395|    200|\n",
      "|    913| 4.338709677419355|     62|\n",
      "|   1221| 4.303703703703704|    135|\n",
      "|     50|4.2835820895522385|    201|\n",
      "|   1252|              4.25|     76|\n",
      "|    904| 4.217391304347826|     92|\n",
      "|    527| 4.209016393442623|    244|\n",
      "|   2019| 4.203703703703703|     54|\n",
      "|   1203| 4.202702702702703|     74|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also see the top 10 rated movies if they have been reviewed at least 50 times or more\n",
    "ratings_sum_df = ratings_df.groupBy('movieId').agg(avg('rating').alias('avg_rating'), count('rating').alias('reviews'))\n",
    "ratings_sum_df.filter(ratings_sum_df.reviews > 50).sort('avg_rating', ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions (UDF)\n",
    "Similar to custom functions for Map, you can write user defined function to transform one or more columns. \n",
    "More about UDF on https://docs.databricks.com/spark/latest/spark-sql/udf-in-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using UDF is a three step process. Before anything you will need to import the udf library\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you can express your user defined function as lambda then you can register the UDF and define it in one line like below\n",
    "# for example this UDF will tell me if I should watch a movie or not based on its average rating\n",
    "watchable_udf = udf(lambda avg_rating: 'yes' if avg_rating > 3.5 else 'no', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise you can first write your function\n",
    "# as you can see here we have more flexibility\n",
    "# I will write the function to also incorporate the total number of reviews\n",
    "def watchable_udf(avg_rating, reviews):\n",
    "  if avg_rating > 3.5 and reviews > 50:\n",
    "    return 'yes'\n",
    "  elif avg_rating > 3.5 and reviews < 50:\n",
    "    return 'maybe'\n",
    "  else:\n",
    "    return 'no'\n",
    "# and then register it as an UDF with the return type declared\n",
    "watchable_udf = udf(watchable_udf, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can use withColumn to apply the UDF over every row and create a new column 'watchable'\n",
    "ratings_sum_df = ratings_sum_df.withColumn('watchable', watchable_udf(ratings_sum_df.avg_rating,ratings_sum_df.reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+---------+\n",
      "|movieId|        avg_rating|reviews|watchable|\n",
      "+-------+------------------+-------+---------+\n",
      "|   2294| 3.150943396226415|     53|       no|\n",
      "|    296| 4.157407407407407|    324|      yes|\n",
      "|   3210|               3.5|     52|       no|\n",
      "|  88140|3.4545454545454546|     22|       no|\n",
      "| 115713| 3.769230769230769|     26|    maybe|\n",
      "|  27317|3.6666666666666665|      6|    maybe|\n",
      "|   1090|3.8088235294117645|     68|      yes|\n",
      "|   2088|              2.52|     25|       no|\n",
      "|   2136| 2.611111111111111|     18|       no|\n",
      "|   6240|               3.0|      2|       no|\n",
      "|   3959|             3.375|     16|       no|\n",
      "|   3606| 4.111111111111111|      9|    maybe|\n",
      "|   6731|3.1666666666666665|      6|       no|\n",
      "|  62912|2.6666666666666665|      3|       no|\n",
      "|  89864|3.5384615384615383|     13|    maybe|\n",
      "| 106022|               4.0|      1|    maybe|\n",
      "|  48738| 3.933333333333333|     15|    maybe|\n",
      "|   2069|3.7777777777777777|      9|    maybe|\n",
      "|   3414|               4.0|      4|    maybe|\n",
      "|   2162|2.5555555555555554|      9|       no|\n",
      "+-------+------------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_sum_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins\n",
    "A JOIN clause is used to combine rows from two or more tables, based on a related column between them. Here are the a few basic types of joins explained:\n",
    "\n",
    "* (INNER) JOIN: Returns records that have matching values in both tables\n",
    "* LEFT (OUTER) JOIN: Return all records from the left table, and the matched records from the right table\n",
    "* RIGHT (OUTER) JOIN: Return all records from the right table, and the matched records from the left table\n",
    "* FULL (OUTER) JOIN: Return all records when there is a match in either left or right table\n",
    "\n",
    "Spark Supports more than just basic joins however. With the latest spark you get: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti joins! Take a look in  https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#join for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|\n",
      "|     18|   Four Rooms (1995)|              Comedy|\n",
      "|     19|Ace Ventura: When...|              Comedy|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets use the movies csv file to make sense of the movies in our previous results\n",
    "movies_df = spark.read.csv(\"/home/fieldengineer/Documents/courses/architect_big_data_solutions_with_spark-master/Datasets/movielens/movies.csv\", header=True)\n",
    "movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do an inner join to get more information about each movies\n",
    "movie_ratings_sum_df = ratings_sum_df.join(movies_df, ratings_sum_df.movieId == movies_df.movieId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-------+---------+\n",
      "|               title|        avg_rating|reviews|watchable|\n",
      "+--------------------+------------------+-------+---------+\n",
      "|         Antz (1998)| 3.150943396226415|     53|       no|\n",
      "| Pulp Fiction (1994)| 4.157407407407407|    324|      yes|\n",
      "|Fast Times at Rid...|               3.5|     52|       no|\n",
      "|Captain America: ...|3.4545454545454546|     22|       no|\n",
      "|   Ex Machina (2015)| 3.769230769230769|     26|    maybe|\n",
      "|Audition (Ã”dishon...|3.6666666666666665|      6|    maybe|\n",
      "|      Platoon (1986)|3.8088235294117645|     68|      yes|\n",
      "|       Popeye (1980)|              2.52|     25|       no|\n",
      "|Nutty Professor, ...| 2.611111111111111|     18|       no|\n",
      "| One Good Cop (1991)|               3.0|      2|       no|\n",
      "|Time Machine, The...|             3.375|     16|       no|\n",
      "|  On the Town (1949)| 4.111111111111111|      9|    maybe|\n",
      "|Day of the Dead (...|3.1666666666666665|      6|       no|\n",
      "|High School Music...|2.6666666666666665|      3|       no|\n",
      "|        50/50 (2011)|3.5384615384615383|     13|    maybe|\n",
      "|Toy Story of Terr...|               4.0|      1|    maybe|\n",
      "|Last King of Scot...| 3.933333333333333|     15|    maybe|\n",
      "|Trip to Bountiful...|3.7777777777777777|      9|    maybe|\n",
      "|Love Is a Many-Sp...|               4.0|      4|    maybe|\n",
      "|NeverEnding Story...|2.5555555555555554|      9|       no|\n",
      "+--------------------+------------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets display a few results\n",
    "movie_ratings_sum_df.select(['title','avg_rating','reviews','watchable']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Can you create a table of the highest rated movie per category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "Spark Lab 6",
  "notebookId": 1713846382052925
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
